{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd7scc-8QJvE"
   },
   "source": [
    "## CS441: Applied ML - HW 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Bmk_x8P4uu"
   },
   "source": [
    "### Part 1: Model Complexity and Tree-based Regressors\n",
    "\n",
    "One measure of a tree’s complexity is the maximum tree depth. Train tree, random forest, and boosted tree regressors on the temperature regression task, using all default parameters except:\n",
    "\n",
    "\n",
    "*   max_depth={2,4,8,16,32}\n",
    "*   random_state=0\n",
    "*   For random forest: max_features=1/3\n",
    "\n",
    "Measure train and val RMSE for each and plot them all on the same plot using the provided plot_depth_error function. You should have six lines (train/val for each model type), each with 5 data points (one for each max depth value).  Include the plot and answer the analysis questions in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2218,
     "status": "ok",
     "timestamp": 1711488553137,
     "user": {
      "displayName": "Noel Mrowiec",
      "userId": "14832469584121835126"
     },
     "user_tz": 300
    },
    "id": "auYU3RgZQSKv",
    "outputId": "8f4d4d70-f8e6-43fe-eb7c-3b846dd922c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load data (modify to match your data directory or comment)\n",
    "def load_temp_data():\n",
    "  #drive.mount('/content/drive')\n",
    "  #datadir = \"/content/drive/My Drive/CS 441/MP4/\"\n",
    "  datadir = \"\"\n",
    "  T = np.load(datadir + 'temperature_data.npz')\n",
    "  x_train, y_train, x_val, y_val, x_test, y_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day = \\\n",
    "  T['x_train'], T['y_train'], T['x_val'], T['y_val'], T['x_test'], T['y_test'], T['dates_train'], T['dates_val'], T['dates_test'], T['feature_to_city'], T['feature_to_day']\n",
    "  return (x_train, y_train, x_val, y_val, x_test, y_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day)\n",
    "\n",
    "# plot one data point for listed cities and target temperature\n",
    "def plot_temps(x, y, cities, feature_to_city, feature_to_day, target_date):\n",
    "  nc = len(cities)\n",
    "  ndays = 5\n",
    "  xplot = np.array([-5,-4,-3,-2,-1])\n",
    "  yplot = np.zeros((nc,ndays))\n",
    "  for f in np.arange(len(x)):\n",
    "    for c in np.arange(nc):\n",
    "      if cities[c]==feature_to_city[f]:\n",
    "        yplot[feature_to_day[f]+ndays,c] = x[f]\n",
    "  plt.plot(xplot,yplot)\n",
    "  plt.legend(cities)\n",
    "  plt.plot(0, y, 'b*', markersize=10)\n",
    "  plt.title('Predict Temp for Cleveland on ' + target_date)\n",
    "  plt.xlabel('Day')\n",
    "  plt.ylabel('Avg Temp (C)')\n",
    "  plt.show()\n",
    "\n",
    "# load data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day) = load_temp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1_zngBjRGji"
   },
   "outputs": [],
   "source": [
    "# to plot the errors\n",
    "def plot_depth_error(max_depths, tree_train_err, tree_val_err, rf_train_err, rf_val_err, bt_train_err, bt_val_err):\n",
    "  plt.figure()\n",
    "  plt.semilogx(max_depths, tree_train_err, 'r.--',label='tree train')\n",
    "  plt.semilogx(max_depths, tree_val_err, 'r.-', label='tree val')\n",
    "  plt.semilogx(max_depths, rf_train_err, 'g.--',label='RF train')\n",
    "  plt.semilogx(max_depths, rf_val_err, 'g.-', label='RF val')\n",
    "  plt.semilogx(max_depths, bt_train_err, 'b.--',label='BT train')\n",
    "  plt.semilogx(max_depths, bt_val_err, 'b.-', label='BT val')\n",
    "  plt.ylabel('RMSE Error')\n",
    "  plt.xlabel('Max Tree Depth')\n",
    "  plt.xticks(max_depths, max_depths)\n",
    "  plt.legend()\n",
    "  plt.rcParams.update({'font.size': 20})\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 352890,
     "status": "ok",
     "timestamp": 1711493614486,
     "user": {
      "displayName": "Noel Mrowiec",
      "userId": "14832469584121835126"
     },
     "user_tz": 300
    },
    "id": "SAxmPVgcQiUI",
    "outputId": "f4d0a714-91f5-483f-fad3-1d881a91f443"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# fit the given model and predicts y for training and evaluation for x_train and x_val\n",
    "def fit_and_predict(model):\n",
    "  model.fit(x_train, y_train)\n",
    "  y_pred_train = model.predict(x_train)\n",
    "  y_pred_val   = model.predict(x_val)\n",
    "  rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "  rmse_val  = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "  return rmse_train, rmse_val\n",
    "\n",
    "max_depths = [2,4,8,16,32]\n",
    "tree_train_error = []\n",
    "tree_val_error = []\n",
    "rf_train_error = []\n",
    "rf_val_error = []\n",
    "bt_train_error = []\n",
    "bt_val_error = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "  model_tree= DecisionTreeRegressor(random_state=0, max_depth=max_depth)\n",
    "  rmse_train_tree, rmse_val_tree = fit_and_predict(model_tree)\n",
    "  tree_train_error.append(rmse_train_tree)\n",
    "  tree_val_error.append(rmse_val_tree)\n",
    "\n",
    "\n",
    "  model_rf = RandomForestRegressor(random_state=0, max_depth=max_depth, max_features=1/3)\n",
    "  rmse_train_rt, rmse_val_rt = fit_and_predict(model_rf)\n",
    "  rf_train_error.append(rmse_train_rt)\n",
    "  rf_val_error.append(rmse_val_rt)\n",
    "\n",
    "  model_bt = GradientBoostingRegressor(random_state=0, max_depth=max_depth)\n",
    "  rmse_train_bt, rmse_val_bt = fit_and_predict(model_bt)\n",
    "  bt_train_error.append(rmse_train_bt)\n",
    "  bt_val_error.append(rmse_val_bt)\n",
    "\n",
    "plot_depth_error(max_depths, tree_train_error, tree_val_error, rf_train_error, rf_val_error, bt_train_error, bt_val_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QagOldZDQJvG"
   },
   "source": [
    "### Part 2: MLPs with MNIST\n",
    "\n",
    "For this part, you will want to use a GPU to improve runtime. Google Colab provides limited free GPU acceleration to all users. Go to Runtime and change Runtime Type to GPU.  This will reset your compute node, so do it before starting to run other cells.\n",
    "\n",
    "See [Tips](https://docs.google.com/document/d/1_kV9x1LCAFfe6UqY22eqj2eqdCZh72oQDf60zRo2ydM/edit?usp=drive_link) for detailed guidance on this problem.\n",
    "\n",
    "First, use PyTorch to implement a Multilayer Perceptron network with one hidden layer (size 64) with ReLU activation. Set the network to minimize cross-entropy loss, which is the negative log probability of the training labels given the training features. This objective function takes unnormalized logits as inputs.\n",
    "\n",
    "*Do not use MLP in sklearn for this HW - use Torch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vO_W4UH7NNBo"
   },
   "outputs": [],
   "source": [
    "# initialization code\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_mnist():\n",
    "  '''\n",
    "  Loads, reshapes, and normalizes the data\n",
    "  '''\n",
    "  (x_train, y_train), (x_test, y_test) = mnist.load_data() # loads MNIST data\n",
    "  x_train = np.reshape(x_train, (len(x_train), 28*28))  # reformat to 768-d vectors\n",
    "  x_test = np.reshape(x_test, (len(x_test), 28*28))\n",
    "  maxval = x_train.max()\n",
    "  x_train = x_train/maxval  # normalize values to range from 0 to 1\n",
    "  x_test = x_test/maxval\n",
    "  return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def display_mnist(x, subplot_rows=1, subplot_cols=1):\n",
    "  '''\n",
    "  Displays one or more examples in a row or a grid\n",
    "  '''\n",
    "  if subplot_rows>1 or subplot_cols>1:\n",
    "    fig, ax = plt.subplots(subplot_rows, subplot_cols, figsize=(15,15))\n",
    "    for i in np.arange(len(x)):\n",
    "      ax[i].imshow(np.reshape(x[i], (28,28)), cmap='gray')\n",
    "      ax[i].axis('off')\n",
    "  else:\n",
    "      plt.imshow(np.reshape(x, (28,28)), cmap='gray')\n",
    "      plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711568790528,
     "user": {
      "displayName": "Noel Mrowiec",
      "userId": "14832469584121835126"
     },
     "user_tz": 300
    },
    "id": "WVt-ck87nUWQ",
    "outputId": "bb8ca518-dcd6-4003-9c1e-b7f0824b84ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Sets device to \"cuda\" if a GPU is available  (in Colabs, enable GPU by Edit->Notebook Settings-->Hardware Accelerator=GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(device) # make sure you're using GPU instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8WbDnbr58LO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2a\n",
    "Using the train/val split provided in the starter code, train your network for 100 epochs with learning rates of 0.01, 0.1, and 1.  Use a batch size of 256 and the SGD optimizer.  After each epoch, record the mean training and validation loss and compute the validation error of the final model. The mean validation loss should be computed after the epoch is complete.  The mean training loss can either be computed after the epoch is complete, or, for efficiency, computed using the losses accumulated during the training of the epoch.  Plot the training and validation losses using the display_error_curves function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2173,
     "status": "ok",
     "timestamp": 1711557707471,
     "user": {
      "displayName": "Noel Mrowiec",
      "userId": "14832469584121835126"
     },
     "user_tz": 300
    },
    "id": "0dwprF51fii8",
    "outputId": "e5bd50cf-4dc7-4380-efcf-a7481154c9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val shape (10000, 784)\n",
      "y_val shape (10000,)\n",
      "x_train shape (50000, 784)\n",
      "y_train shape (50000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "\n",
    "# create train/val split\n",
    "ntrain = 50000\n",
    "x_val = x_train[ntrain:].copy()\n",
    "y_val = y_train[ntrain:].copy()\n",
    "x_train = x_train[:ntrain]\n",
    "y_train = y_train[:ntrain]\n",
    "print(f\"x_val shape {x_val.shape}\")\n",
    "print(f\"y_val shape {y_val.shape}\")\n",
    "print(f\"x_train shape {x_train.shape}\")\n",
    "print(f\"y_train shape {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xBVdHhhB8UMl"
   },
   "outputs": [],
   "source": [
    "def display_error_curves(training_losses, validation_losses):\n",
    "  \"\"\"\n",
    "  Plots the training and validation loss curves\n",
    "  training_losses and validation_losses should be lists or arrays of the same length\n",
    "  \"\"\"\n",
    "  num_epochs = len(training_losses)\n",
    "\n",
    "  plt.plot(range(num_epochs), training_losses, label=\"Training Loss\")\n",
    "  plt.plot(range(num_epochs), validation_losses, label=\"Validation Loss\")\n",
    "\n",
    "  # Add in a title and axes labels\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "\n",
    "  # Display the plot\n",
    "  plt.legend(loc='best')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yLhPolEhEdrt"
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        #super(MLP, self).__init__()\n",
    "        #should be super().__init__() in python3\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = nn.functional.relu(self.hidden_layer(x))\n",
    "        x = self.hidden_layer(x)  # Get intermediate outputs using hidden layer\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.output(x)    # Get predictions using output layer\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yxyQ6-ZwEdrt"
   },
   "outputs": [],
   "source": [
    "# This is a possible function definition for training MLP, but feel free to change it\n",
    "# You may also want to create helper functions, e.g. for computing loss or prediction\n",
    "def train_MLP_mnist(train_loader, val_loader, lr=1e-1, num_epochs=100):\n",
    "  '''\n",
    "  Train a MLP\n",
    "  Input: train_loader and val_loader are dataloaders for the training and\n",
    "  val data, respectively. lr is the learning rate, and the network will\n",
    "  be trained for num_epochs epochs.\n",
    "  Output: return a trained MLP\n",
    "  '''\n",
    "  # TODO: fill in all code\n",
    "\n",
    "  input_size = 784\n",
    "  hidden_size = 64\n",
    "  output_size = 10\n",
    "\n",
    "  # Instantiate the model\n",
    "  mlp = MLP(input_size, hidden_size, output_size).to(device)\n",
    "  #mlp = MLP(input_size, hidden_size, output_size)\n",
    "  #print(f\"Model is currently on device: {mlp.device}\")\n",
    "\n",
    "  # Train the model, compute and store train/val loss at each epoch\n",
    "  optimizer = torch.optim.SGD(mlp.parameters(), lr=lr)\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "  train_loss_list = []\n",
    "  val_loss_list = []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # Iterate over the DataLoader for training data\n",
    "    sum_train_loss = 0\n",
    "    sum_val_loss = 0\n",
    "    total_train_samples = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "      inputs, targets = inputs.to(device), targets.to(device)\n",
    "      optimizer.zero_grad() # Zero the gradients\n",
    "      outputs = mlp(inputs) # Compute logit scores for current batch\n",
    "      train_loss = loss_function(outputs, targets) # Compute loss\n",
    "      sum_train_loss += train_loss.item() * inputs.size(0)\n",
    "      total_train_samples += inputs.size(0)\n",
    "      #sum_train += train_loss.item()\n",
    "      train_loss.backward() # Backprop loss\n",
    "      optimizer.step() # Update weights\n",
    "    with torch.no_grad():\n",
    "      for inputs, targets in val_loader:   \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = mlp(inputs) # Compute logit scores for current batch\n",
    "        val_loss = loss_function(outputs, targets) # Compute loss\n",
    "        sum_val_loss += val_loss.item() * inputs.size(0)\n",
    "        total_val_samples += inputs.size(0)\n",
    "\n",
    "    mean_train_loss = sum_train_loss / total_train_samples\n",
    "    mean_val_loss = sum_val_loss / total_val_samples\n",
    "    train_loss_list.append(mean_train_loss)\n",
    "    val_loss_list.append(mean_val_loss)\n",
    "\n",
    "  # Display Loss Curves\n",
    "  print(f\"Learning rate = {lr}\")\n",
    "  print(f\"Model's min loss index={val_loss_list.index(min(val_loss_list))}, and value {min(val_loss_list)}\")\n",
    "  display_error_curves(train_loss_list, val_loss_list)\n",
    "  return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MLP(mlp, loader):\n",
    "  ''' Computes loss and error rate given your mlp model and data loader'''\n",
    "  N = 0\n",
    "  acc = 0\n",
    "  loss = 0\n",
    "  loss_function = torch.nn.CrossEntropyLoss()\n",
    "  with torch.set_grad_enabled(False):\n",
    "    for i, data in enumerate(loader, 0):\n",
    "\n",
    "      # Get inputs\n",
    "      inputs, targets = data\n",
    "      N += len(targets)\n",
    "\n",
    "      # Perform forward pass\n",
    "      outputs = mlp(inputs.to(device))\n",
    "\n",
    "      # Compute sum of correct labels\n",
    "      y_pred = np.argmax(outputs.cpu().numpy(), axis=1)\n",
    "      y_gt = np.argmax(targets.numpy(), axis=1)\n",
    "      acc += np.sum(y_pred==y_gt)\n",
    "\n",
    "      # Compute loss\n",
    "      loss += loss_function(outputs, targets.to(device)).item()*len(targets)\n",
    "\n",
    "  loss /= N\n",
    "  acc /= N\n",
    "\n",
    "  return loss, 1-acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 290998,
     "status": "error",
     "timestamp": 1711564539852,
     "user": {
      "displayName": "Noel Mrowiec",
      "userId": "14832469584121835126"
     },
     "user_tz": 300
    },
    "id": "v7ElMBso4m9z",
    "outputId": "337de9b1-0dea-4efe-f05b-2a23da5d6e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Code for running experiments\n",
    "\n",
    "\n",
    "print(device) # make sure you're using GPU instance\n",
    "torch.manual_seed(0) # to avoid randomness, but if you wanted to create an ensemble, you should not use a manual seed\n",
    "batch_size = 256  \n",
    "\n",
    "# TODO (set up dataloaders, and call training function)\n",
    "#trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(np.eye(10)[y_train]).to(device))\n",
    "train_set = torch.utils.data.TensorDataset(torch.Tensor(x_train), torch.Tensor(np.eye(10)[y_train]))\n",
    "#train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "#eval_set = torch.utils.data.TensorDataset(torch.Tensor(x_val).to(device), torch.Tensor(np.eye(10)[y_val]).to(device))\n",
    "eval_set = torch.utils.data.TensorDataset(torch.Tensor(x_val), torch.Tensor(np.eye(10)[y_val]))\n",
    "#eval_loader = torch.utils.data.DataLoader(trainset, batch_size=2*batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_set, batch_size=2*batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "learing_rate = [0.001, 0.01, 0.1, 1]\n",
    "mlp_models = []\n",
    "for lr in learing_rate:\n",
    "  mlp_models.append(train_MLP_mnist(train_loader, eval_loader, lr=lr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5sScSiG6K_0"
   },
   "source": [

    "#### 2b\n",
    "Based on the loss curves, select the learning rate and number of epochs that minimizes the validation loss.  Retrain that model (if it's not stored), and report training loss, validation loss, training error, validation error, and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RoeE5BxXAqqi"
   },
   "outputs": [],
   "source": [
    "# TO DO (retrain if needed, and evaluate model on train, val, and test sets)\n",
    "lr = 0.1    #from the print of above\n",
    "epochs = 85 #from the print of above\n",
    "\n",
    "#testset = torch.utils.data.TensorDataset(torch.Tensor(x_test), torch.Tensor(np.eye(10)[y_test]))\n",
    "#test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mlp = train_MLP_mnist(train_loader, eval_loader, lr=lr, num_epochs=epochs)\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(torch.Tensor(x_val), torch.Tensor(np.eye(10)[y_val]))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2*batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "val_loss, val_err = evaluate_MLP(mlp, eval_loader)\n",
    "test_loss, test_err = evaluate_MLP(mlp, test_loader)\n",
    "train_loss, train_err = evaluate_MLP(mlp, train_loader)\n",
    "print(f\"validation loss: {val_loss}, validation error: {val_err}\")\n",
    "print(f\"test loss: {test_loss}, test error: {test_err}\")\n",
    "print(f\"train loss: {train_loss}, train error: {train_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F49MYTryhPJB"
   },
   "source": [
    "## Part 3: Predicting Penguin Species\n",
    "\n",
    "Include all your code for part 3 in this section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u9i_hB-UQWDp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#styling preferences for sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('poster')\n",
    "#drive.mount('/content/gdrive/')\n",
    "#datadir = \"/content/gdrive/MyDrive/CS441/hw4/\" # TO DO: modify this to your directory\n",
    "datadir = \"\" # TO DO: modify this to your directory\n",
    "df_penguins = pd.read_csv(datadir + 'penguins_size.csv')\n",
    "df_penguins.head(10)\n",
    "\n",
    "# convert features with multiple string values to binary features so they can be used by sklearn\n",
    "def get_penguin_xy(df_penguins):\n",
    "  data = np.array(df_penguins[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']])\n",
    "  y = df_penguins['species']\n",
    "  ui = np.unique(data[:,0]) # unique island\n",
    "  us = np.unique(data[:,-1]) # unique sex\n",
    "  X = np.zeros((len(y), 10))\n",
    "  for i in range(len(y)):\n",
    "    f = 0\n",
    "    for j in range(len(ui)):\n",
    "      if data[i, f]==ui[j]:\n",
    "        X[i, f+j] = 1\n",
    "    f = f + len(ui)\n",
    "    X[i, f:(f+4)] = data[i, 1:5]\n",
    "    f=f+4\n",
    "    for j in range(len(us)):\n",
    "      if data[i, 5]==us[j]:\n",
    "        X[i, f+j] = 1\n",
    "  feature_names = ['island_biscoe', 'island_dream', 'island_torgersen', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex_female', 'sex_male', 'sex_unknown']\n",
    "  X = pd.DataFrame(X, columns=feature_names)\n",
    "  return(X, y, feature_names, np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeZciiipC3tH"
   },
   "source": [
    "#### 3a\n",
    "Spend some time to visualize different pairs of features and their relationships to the species.  We’ve done one for you.  Include in your report at least two other visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AGd0yEc_SINQ"
   },
   "outputs": [],
   "source": [
    "def plot_scatter(feature1, feature2):\n",
    "  '''\n",
    "  Provide names of two features to create a scatterplot of them\n",
    "  E.g. plot_scatter('culmen_length_mm', 'culmen_depth_mm')\n",
    "  Possible features: 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "  '''\n",
    "\n",
    "  palette = [\"red\", \"blue\", \"orange\"]\n",
    "\n",
    "  sns.scatterplot(data=df_penguins, x = feature1, y = feature2,\n",
    "               hue = 'species', palette=palette, alpha=0.8)\n",
    "  # Doc: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "\n",
    "  plt.xlabel(feature1, fontsize=14)\n",
    "  plt.ylabel(feature2, fontsize=14)\n",
    "  plt.title(feature1 + ' vs ' + feature2, fontsize=20)\n",
    "  plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "# TO DO call plot_scatter with different feature pairs to create some visualizations\n",
    "\n",
    "plot_scatter('culmen_length_mm', 'culmen_depth_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwLC5rpmDDlp"
   },
   "source": [
    "#### 3b\n",
    "Suppose you want to be able to identify the Gentoo species with a simple rule with very high accuracy.  Use a decision tree classifier to figure out such a rule that has only two checks (e.g. “mass greater than 4000 g, and culmen length less than 40 mm is Gentoo; otherwise, not”).   You can use the library DecisionTreeClassifier with either ‘gini’ or ‘entropy’ criterion. Use sklearn.tree.plot_tree with feature_names and class_names arguments to visualize the decision tree.  Include the tree that you used to find the rule in your report and the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BEXQoDjDDPF"
   },
   "outputs": [],
   "source": [
    "# TO DO (Train a short tree to identify a good rule, plot thetree, report the rule and its precision/recall in your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RIRmYltDHZV"
   },
   "source": [
    "#### 3c\n",
    "\n",
    "Use any method at your disposal to achieve maximum 5-fold cross-validation accuracy on this problem. To keep it simple, we will use sklearn.model_selection to perform the cross-validation for us. Report your model design and 5-fold accuracy.  It is possible to get more than 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qQ5TKOVS6DE"
   },
   "outputs": [],
   "source": [
    "# design a classification model, import libraries as needed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y, feature_names, class_names = get_penguin_xy(df_penguins)\n",
    "\n",
    "# TO DO -- choose some model and fit the data\n",
    "model = ...\n",
    "\n",
    "scores = cross_val_score(model, np.array(X), np.array(y), cv=5)\n",
    "print('CV Accuracy: {}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3X3j_efPhh6e"
   },
   "source": [
    "## Part 4: Stretch Goals\n",
    "Include any new code needed for Part 4 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5SnDgjKVdLJ"
   },
   "outputs": [],
   "source": [
    "# TO DO (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwOqlBwgEJfa"
   },
   "outputs": [],
   "source": [
    "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
    "# install can take a minute\n",
    "\n",
    "import os\n",
    "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
    "NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/24SP/hw2\" # @param {type:\"string\"}\n",
    "NOTEBOOK_NAME = \"CS441_SP24_HW2_Solution.ipynb\" # @param {type:\"string\"}\n",
    "#------------------------------------------------------------------------------#\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\", force_remount=True)\n",
    "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
    "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
    "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
    "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
    "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
    "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
    "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"gg\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   